""" Defines modality-specific encoders and joint multimodal model. """

import torch
import torch.nn as nn
import torchvision
import torch.nn.functional as F

from config import config

class Encoder(nn.Module):
    """ Encoder model for a specific modality (DaTSCAN or MRI). """
    def __init__(self, modality):
        super(Encoder, self).__init__()
        if modality == 'datscan':
            self.enc_backbone = config.DATSCAN_ENC
            # self.model = getattr(torchvision.models, self.enc_backbone)(weights = 'IMAGENET1K_V1')
            self.model = getattr(torchvision.models, self.enc_backbone)(pretrained=True)
        elif modality == 'mri':
            self.enc_backbone = config.MRI_ENC
            # self.model = getattr(torchvision.models, self.enc_backbone)(weights = 'IMAGENET1K_V1')
            self.model = getattr(torchvision.models, self.enc_backbone)(pretrained=True)

        # remove final classifier layer from encoder backbone, in order to use features generated by backbone
        classifiers = ['classifier', 'fc']
        for classifier in classifiers:
            cls_layer = getattr(self.model, classifier, None)
            if cls_layer is None:
                continue
            # for EfficientNet, classifier is a Sequential layer so select in_features from penultimate layer
            if self.enc_backbone.startswith('efficientnet') and isinstance(cls_layer, nn.Sequential):
                d_visual = cls_layer[1].in_features
            # for ResNet and InceptionV3, classifier is simply a Linear layer
            else:
                d_visual = cls_layer.in_features
            setattr(self.model, classifier, nn.Identity())
            break
        
        self.feats_dim = d_visual

        if config.FREEZE_ENCS:
            for p in self.model.parameters():
                p.requires_grad = False

    def forward(self, x):
        # InceptionV3 returns InceptionOutputs in train mode so extract logits tensor specifically
        if self.enc_backbone.startswith('inception'):
            out = self.model(x)
            feats = out.logits if hasattr(out, 'logits') else out
        else:
            feats = self.model(x)
        return feats

class JointModel(nn.Module):
    """ Joint multimodal model. """
    def __init__(self, datscan_enc, mri_enc):
        super(JointModel, self).__init__()
        self.datscan_enc = datscan_enc
        self.mri_enc = mri_enc

        # if feature dimensions differ, add a projection layer to align them
        datscan_dim = self.datscan_enc.feats_dim
        mri_dim = self.mri_enc.feats_dim
        if datscan_dim < mri_dim:
            self.projection = nn.Linear(datscan_dim, mri_dim)    # project DaTSCAN feature repr. to MRI feature repr. space
            self.proj_modality = 'datscan'
            feats_dim = 2 * mri_dim
        elif mri_dim < datscan_dim:
            self.projection = nn.Linear(mri_dim, datscan_dim)    # project MRI feature repr. to DaTSCAN feature repr. space
            self.proj_modality = 'mri'
            feats_dim = 2 * datscan_dim
        else:
            self.projection = None
            self.proj_modality = None
            feats_dim = datscan_dim + mri_dim

        self.fused_classifier = nn.Sequential(
            nn.Linear(feats_dim, feats_dim),
            nn.ReLU(),
            nn.Linear(feats_dim, 1)
        )

        self.sigmoid = nn.Sigmoid()

    def forward(self, datscan, mri):
        datscan_feats = self.datscan_enc(datscan)
        mri_feats = self.mri_enc(mri)
        
        if self.proj_modality == 'datscan':
            datscan_feats = self.projection(datscan_feats)
        elif self.proj_modality == 'mri':
            mri_feats = self.projection(mri_feats)
        elif self.proj_modality is None:
            pass

        feats = torch.cat([datscan_feats, mri_feats], dim=1)
        fused_logits = self.fused_classifier(feats)
        fused_preds = self.sigmoid(fused_logits)
        
        ret = {
            'logits': fused_logits,     # for BCEWithLogitsLoss, shape: (batch_size, 1)
            'preds': fused_preds,       # model predictions, shape: (batch_size, 1)
            'datscan_feats': datscan_feats,
            'mri_feats': mri_feats
        }
        return ret

class JointAttnModel(nn.Module):
    """ Joint multimodal model with spatial self-attention over feature maps (ResNet-18/34 only)

    For each modality:
      - Use the ResNet backbone's conv features before global pooling
      - Produce an attention map over spatial locations (H x W)
      - Use the attention map to compute a weighted global feature vector
    Concatenate the two modality-specific global features and classify

    Returns attention maps that can be upsampled to input size for interpretable heatmaps """

    def __init__(self, datscan_enc, mri_enc, attn_dim=256):
        super().__init__()
        self.datscan_enc = datscan_enc
        self.mri_enc = mri_enc
        self.attn_dim = attn_dim

        # feature extractors
        self.ds_feat_extractor, ds_channels = self._build_feat_extr(self.datscan_enc.model, self.datscan_enc.feats_dim)
        self.mri_feat_extractor, mri_channels = self._build_feat_extr(self.mri_enc.model, self.mri_enc.feats_dim)

        # projection layers to attn_dim
        self.ds_proj = nn.Conv2d(ds_channels, attn_dim, kernel_size=1)
        self.mri_proj = nn.Conv2d(mri_channels, attn_dim, kernel_size=1)

        # self-attention heads: produce spatial attention logits (B, 1, H, W)
        self.ds_attn_head = nn.Conv2d(attn_dim, 1, kernel_size=1)
        self.mri_attn_head = nn.Conv2d(attn_dim, 1, kernel_size=1)

        # classifier on fused DaTSCAN + MRI global features
        fused_dim = 2 * attn_dim
        self.classifier = nn.Sequential(
            nn.Linear(fused_dim, fused_dim),
            nn.ReLU(),
            nn.Linear(fused_dim, 1),
        )
        self.sigmoid = nn.Sigmoid()

        if config.FREEZE_ENCS:
            for p in self.ds_feat_extractor.parameters():
                p.requires_grad = False
            for p in self.mri_feat_extractor.parameters():
                p.requires_grad = False

    def _build_feat_extr(self, backbone, channels):
        """ Returns feature extractor (backbone up to layer4) and number of output channels """
        feat_extractor = nn.Sequential(
            backbone.conv1,
            backbone.bn1,
            backbone.relu,
            backbone.maxpool,
            backbone.layer1,
            backbone.layer2,
            backbone.layer3,
            backbone.layer4,
        )
        # channels is 512 for ResNet-18/34, 2048 for ResNet-50
        return feat_extractor, channels

    def _perform_attn(self, feat_map, proj_layer, attn_head):
        """ feat_map: (B, C_in, H, W)
            proj_layer: Conv2d C_in -> C
            attn_head: Conv2d C -> 1

            Returns:
            global_feat: (B, C)
            attn_map: (B, 1, H, W) softmax over H*W """
        x = proj_layer(feat_map)  # (B, C, H, W)

        # attention logits and weights
        attn_logits = attn_head(x)                          # (B, 1, H, W)
        B, _, H, W = attn_logits.shape
        attn_flat = attn_logits.view(B, -1)                 # (B, H*W)

        attn_weights = F.softmax(attn_flat, dim=1)          # (B, H*W)
        attn_map = attn_weights.view(B, 1, H, W)            # (B, 1, H, W)

        # weighted global pooling
        x_flat = x.view(B, x.shape[1], -1)                  # (B, C, H*W)
        attn_flat = attn_weights.unsqueeze(1)               # (B, 1, H*W)
        global_feat = torch.sum(x_flat * attn_flat, dim=2)  # (B, C)

        return global_feat, attn_map

    def forward(self, datscan, mri):
        # extract spatial feature maps
        ds_map = self.ds_feat_extractor(datscan)            # (B, C_ds, H_ds, W_ds)
        mri_map = self.mri_feat_extractor(mri)              # (B, C_mri, H_mri, W_mri)

        # modality-specific attention and global features
        ds_global, ds_attn = self._perform_attn(ds_map, self.ds_proj, self.ds_attn_head)
        mri_global, mri_attn = self._perform_attn(mri_map, self.mri_proj, self.mri_attn_head)

        # fuse global features and classify
        fused = torch.cat([ds_global, mri_global], dim=1)  # (B, 2*attn_dim)
        logits = self.classifier(fused)                    # (B, 1)
        preds = self.sigmoid(logits)                       # (B, 1)

        return {
            "logits": logits,
            "preds": preds,
            "datscan_attn": ds_attn,                       # (B, 1, H, W)
            "mri_attn": mri_attn,                          # (B, 1, H, W)
            "datscan_global": ds_global,
            "mri_global": mri_global,
        }

class UnimodalModel(nn.Module):
    """ Unimodal baseline: one encoder + classifier head. """
    def __init__(self, enc, modality):
        super(UnimodalModel, self).__init__()
        self.enc = enc
        self.modality = modality

        feats_dim = self.enc.feats_dim
        self.classifier = nn.Sequential(
            nn.Linear(feats_dim, feats_dim),
            nn.ReLU(),
            nn.Linear(feats_dim, 1),
        )
        self.sigmoid = nn.Sigmoid()

    def forward(self, datscan, mri):
        # use only one modality; ignore the other
        if self.modality == "datscan":
            feats = self.enc(datscan)
        elif self.modality == "mri":
            feats = self.enc(mri)

        logits = self.classifier(feats)
        preds = self.sigmoid(logits)

        return {
            "logits": logits,
            "preds": preds,
            "datscan_feats": feats if self.modality == "datscan" else None,
            "mri_feats": feats if self.modality == "mri" else None,
        }